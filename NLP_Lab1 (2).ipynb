{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Z0ruB2HwH6"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XUqb_iKHwH8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlLJa6DAHwH9",
        "outputId": "a0ece546-0118-487e-ab3e-0e4c1e4bbcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('movie_reviews')  # Download movie reviews dataset\n",
        "nltk.download('punkt')  # Required for tokenization later\n",
        "nltk.download('stopwords')  # Required for stopwords removal\n",
        "nltk.download('wordnet')  # Required for lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9aPkxgzHwH-"
      },
      "outputs": [],
      "source": [
        "# Load movie reviews dataset\n",
        "file_ids = movie_reviews.fileids()  # Get all document IDs\n",
        "\n",
        "# Extract first 5 positive and negative reviews\n",
        "positive_reviews = [movie_reviews.raw(file_id) for file_id in file_ids if file_id.startswith('pos')][:5]\n",
        "negative_reviews = [movie_reviews.raw(file_id) for file_id in file_ids if file_id.startswith('neg')][:5]\n",
        "\n",
        "# Combine them into a dataset\n",
        "dataset = positive_reviews + negative_reviews\n",
        "labels = ['positive'] * 5 + ['negative'] * 5  # Assign labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woCxmmTwHwH-",
        "outputId": "80afbadf-fa31-457e-8665-43f25cbbc0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative']\n"
          ]
        }
      ],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1zD51awHwH_",
        "outputId": "ab4b68ff-7682-4e3c-9724-e0651962fc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Text Sample:\n",
            "\n",
            "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
            "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
            "to say moore and campbell thoroughly researched the subject\n"
          ]
        }
      ],
      "source": [
        "# Print the first 500 characters of the first review\n",
        "print(\"Raw Text Sample:\\n\")\n",
        "print(dataset[0][:500])  # Show first 500 characters of the first review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZe9JBX-HwH_"
      },
      "source": [
        "## 1. Basic Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxCb3bP3HwH_",
        "outputId": "28c68d7a-3efe-45be-9a40-1d1005c45acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru7FRP4dHwH_"
      },
      "source": [
        "### 1.1. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ97VMtvHwH_",
        "outputId": "743a6134-cced-499c-b57c-2c9252a3b568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Word Tokenization:\n",
            " ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'re\", 'about', 'superheroes', '(', 'batman', ',', 'superman']\n",
            "\n",
            " Sentence Tokenization:\n",
            " [\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\", \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\", 'to say moore and campbell thoroughly researched the subject']\n",
            "\n",
            " Subword Tokenization:\n",
            " ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superhero', '##es', '(', 'batman']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Hugging Face tokenizer for subword tokenization\n",
        "bpe_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Sample text for tokenization\n",
        "sample_text = dataset[0][:500]  # First 500 characters of first review\n",
        "\n",
        "## 1️ Word Tokenization using NLTK\n",
        "word_tokens = nltk.word_tokenize(sample_text)\n",
        "print(\" Word Tokenization:\\n\", word_tokens[:20])  # Print first 20 tokens\n",
        "\n",
        "## 2️ Sentence Tokenization using NLTK\n",
        "sent_tokens = nltk.sent_tokenize(sample_text)\n",
        "print(\"\\n Sentence Tokenization:\\n\", sent_tokens[:3])  # Print first 3 sentences\n",
        "\n",
        "## 3️ Subword Tokenization using WordPiece (BERT)\n",
        "subword_tokens = bpe_tokenizer.tokenize(sample_text)\n",
        "print(\"\\n Subword Tokenization:\\n\", subword_tokens[:20])  # Print first 20 subword tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOo6WxH4HwIA",
        "outputId": "9a8a1043-d146-4f91-c747-4890c8b4c708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
            "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
            "to say moore and campbell thoroughly researched the subject\n"
          ]
        }
      ],
      "source": [
        "print(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGVIVMCMHwIA"
      },
      "source": [
        "### 1.2. Stemming vs. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwMD-6NrHwIA",
        "outputId": "4173c494-f847-49ed-8bcd-c6a2c7517487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Stemmed Words: ['film', 'adapt', 'from', 'comic', 'book', 'have', 'had', 'plenti', 'of', 'success', ',', 'whether', 'they', \"'re\", 'about', 'superhero', '(', 'batman', ',', 'superman']\n",
            "\n",
            " Lemmatized Words: ['film', 'adapted', 'from', 'comic', 'book', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'re\", 'about', 'superheroes', '(', 'batman', ',', 'superman']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example tokens\n",
        "tokens = nltk.word_tokenize(sample_text)\n",
        "\n",
        "# Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"\\n Stemmed Words:\", stemmed_words[:20])  # Print first 20 stemmed words\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"\\n Lemmatized Words:\", lemmatized_words[:20])  # Print first 20 lemmatized words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKMfZ4umHwIB"
      },
      "source": [
        "### 1.3. Removal of stop words, special characters, and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokens\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"\\n Filtered Tokens (Stopwords removed):\", filtered_tokens[:20])  # First 20 filtered tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiSt1OSPIMDX",
        "outputId": "071866cc-dafa-41e1-8e3c-71e45c1b0ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Filtered Tokens (Stopwords removed): ['films', 'adapted', 'comic', 'books', 'plenty', 'success', ',', 'whether', \"'re\", 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'geared', 'toward']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0kBfnk0HwIB",
        "outputId": "974a6ebb-965a-49f5-9ff7-c24dea561785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Cleaned Text (No Punctuation): films adapted from comic books have had plenty of success  whether theyre about superheroes  batman \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Remove special characters and punctuation using regex\n",
        "clean_text = re.sub(r'[^\\w\\s]', '', sample_text)\n",
        "print(\"\\n Cleaned Text (No Punctuation):\", clean_text[:100])  # Print first 100 characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxnEoKOMHwIB",
        "outputId": "749f949a-6a35-4128-e7f3-5741aee50b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Cleaned Text (Lowercased and Whitespace Normalized): films adapted from comic books have had plenty of success whether theyre about superheroes batman su\n"
          ]
        }
      ],
      "source": [
        "# Lowercase the text\n",
        "cleaned_text = clean_text.lower()\n",
        "\n",
        "# Normalize whitespace\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(\"\\n Cleaned Text (Lowercased and Whitespace Normalized):\", cleaned_text[:100])  # Print first 100 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnplpIGjHwIC"
      },
      "source": [
        "## 2. Context-Aware Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN75UhahHwIC"
      },
      "source": [
        "### 2.1 Sentence segmentation using nltk.sent_tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpn5PjgKHwIC",
        "outputId": "07e6549b-0045-4af4-e754-1b3b225df25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sentence Segmentation: [\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\", \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\", 'to say moore and campbell thoroughly researched the subject']\n"
          ]
        }
      ],
      "source": [
        "# Sentence segmentation using nltk\n",
        "sent_tokens = nltk.sent_tokenize(sample_text)\n",
        "print(\"\\n Sentence Segmentation:\", sent_tokens[:3])  # Print first 3 sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nytGU22HwIC"
      },
      "source": [
        "### 2.2 Handling out-of-vocabulary (OOV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C2hZ0c5HwIC",
        "outputId": "b0a8ff57-9b2d-4f97-a55f-c048803e3bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Subword Tokenization (WordPiece): ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superhero', '##es', '(', 'batman']\n"
          ]
        }
      ],
      "source": [
        "# Subword tokenization using WordPiece (BERT)\n",
        "subword_tokens = bpe_tokenizer.tokenize(sample_text)\n",
        "print(\"\\n Subword Tokenization (WordPiece):\", subword_tokens[:20])  # Print first 20 subwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZosv-kvHwIC"
      },
      "source": [
        "### 2.3 Named Entity Removal/Replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aQMt7LYHwIC",
        "outputId": "f366a40e-ce0a-4f6d-b197-ff30cfc4c657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Named Entities: [('alan moore', 'PERSON'), ('eddie campbell', 'PERSON'), (\"the mid '80s\", 'DATE'), ('12', 'CARDINAL')]\n",
            "\n",
            " Anonymized Text (Named Entities Removed): films adapted from comic books have had plenty of success , whether they're about superheroes ( batm\n"
          ]
        }
      ],
      "source": [
        "# Named Entity Recognition using spaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Extract named entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\n Named Entities:\", entities)\n",
        "\n",
        "# Replace named entities with a placeholder\n",
        "anonymized_text = sample_text\n",
        "for ent in doc.ents:\n",
        "    anonymized_text = anonymized_text.replace(ent.text, '[REDACTED]')\n",
        "\n",
        "print(\"\\n Anonymized Text (Named Entities Removed):\", anonymized_text[:100])  # Print first 100 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90JWs8wOHwIC"
      },
      "source": [
        "## 3. Text Representation Techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11cgUhBLHwIC"
      },
      "source": [
        "### 3.1. One-hot encoding vs. TF-IDF vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYm_w0feHwID",
        "outputId": "82986167-3f80-4f17-ed25-8a8e39ae6fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " One-Hot Encoding Matrix:\n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            " TF-IDF Encoding Matrix:\n",
            " [[0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.18814417\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.18814417 0.09407209\n",
            "  0.18814417 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.18814417 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.18814417 0.09407209 0.09407209 0.09407209 0.18814417\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.47036043 0.09407209 0.09407209 0.09407209 0.18814417\n",
            "  0.09407209 0.09407209 0.09407209 0.09407209 0.09407209 0.09407209\n",
            "  0.09407209 0.09407209]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# One-hot encoding using CountVectorizer\n",
        "one_hot_vectorizer = CountVectorizer(binary=True)\n",
        "one_hot_matrix = one_hot_vectorizer.fit_transform([sample_text])\n",
        "print(\"\\n One-Hot Encoding Matrix:\\n\", one_hot_matrix.toarray())\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([sample_text])\n",
        "print(\"\\n TF-IDF Encoding Matrix:\\n\", tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHmnGzKLHwID",
        "outputId": "201110de-8c65-4177-ab08-0696d649d2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Word2Vec Embedding for 'films': [ 1.07737314e-02  1.96202751e-02 -1.41233625e-02 -1.15341647e-03\n",
            " -1.01341652e-02  1.28713986e-02  1.93710309e-02 -6.50823175e-04\n",
            "  1.02639096e-02  1.30786821e-02  8.82290676e-03 -1.60772242e-02\n",
            "  6.22067926e-03 -2.96727149e-03 -4.34686057e-03  1.08161494e-02\n",
            " -2.60245777e-03  1.98630840e-02 -1.26340566e-03  5.00693568e-04\n",
            " -4.53380123e-03 -1.70178674e-02 -1.95871275e-02  9.91012901e-03\n",
            " -1.32186934e-02  5.93876373e-03  5.54669602e-03 -1.39798019e-02\n",
            " -6.28188392e-03 -1.16151609e-02 -1.71332825e-02  7.85340017e-05\n",
            "  1.95950866e-02 -1.33755216e-02 -8.73577408e-03  1.66702867e-02\n",
            "  1.86194703e-02  1.96353644e-02 -1.97834000e-02  1.01065747e-02\n",
            " -1.32795414e-02  1.37526048e-02  1.23024751e-02 -6.75472431e-03\n",
            "  1.38038322e-02 -1.57125741e-02  9.08832345e-03  9.13464371e-03\n",
            "  1.18486714e-02  1.17653031e-02]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the text into words for Word2Vec\n",
        "tokens = nltk.word_tokenize(sample_text)\n",
        "\n",
        "# Train a Word2Vec model (for demonstration, use a small corpus)\n",
        "model = Word2Vec([tokens], vector_size=50, window=3, min_count=1, workers=4)\n",
        "\n",
        "# Get the vector for a word\n",
        "word_vector = model.wv['films']\n",
        "print(\"\\n Word2Vec Embedding for 'films':\", word_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOweAazwHwID",
        "outputId": "0b14ff4b-99ed-4db5-8c74-ca276df0f938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " FastText Embedding for 'test': [ 0.0026845   0.00407108 -0.00730213 -0.00144425  0.00205896  0.00120375\n",
            "  0.00349664 -0.00345134  0.00588176 -0.00809247 -0.00343208 -0.00291846\n",
            "  0.00538609  0.00085953  0.00343121  0.00281992  0.00482496  0.00203102\n",
            "  0.00488304 -0.00669601 -0.00048246 -0.0008616  -0.00328265  0.00180765\n",
            " -0.00228827  0.00096822  0.00112713 -0.00454078 -0.00231561  0.00171805\n",
            " -0.00447071  0.00194817  0.00053104  0.00274572  0.00465155  0.00819038\n",
            "  0.00328887 -0.00212494  0.00147701 -0.00250585 -0.00107412 -0.00719092\n",
            " -0.00241617 -0.00178923 -0.00010647  0.00439559  0.00297278 -0.00907217\n",
            "  0.0088592   0.00524733]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train FastText model on the same tokenized text\n",
        "fasttext_model = FastText(sentences=[tokens], vector_size=50, window=3, min_count=1, sg=1)\n",
        "\n",
        "# Get the vector for a word\n",
        "fasttext_word_vector = fasttext_model.wv['test']\n",
        "\n",
        "# Output the FastText word vector for 'test'\n",
        "print(\"\\n FastText Embedding for 'test':\", fasttext_word_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXinyHICLm4L",
        "outputId": "7fb14daa-0790-4d6d-9bd3-8b1320d0b552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 18:17:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-04-18 18:17:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-04-18 18:17:09--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2025-04-18 18:19:48 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Path to the downloaded GloVe file\n",
        "glove_input_file = '/content/glove.6B.50d.txt'  # Adjust this path\n",
        "word2vec_output_file = 'glove.6B.50d.w2vformat.txt'\n",
        "\n",
        "# Convert GloVe to Word2Vec format\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcd9z2haLSxc",
        "outputId": "73e54e35-2e34-434b-a319-e371b006e206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-79fddd60410c>:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Load pre-trained GloVe model (download a GloVe model, e.g., from Stanford)\n",
        "# Here, we load it as a word2vec format model for compatibility with Gensim\n",
        "glove_model = KeyedVectors.load_word2vec_format('glove.6B.50d.w2vformat.txt', binary=False)\n",
        "\n",
        "# Get the vector for a word (example: 'test')\n",
        "glove_word_vector = glove_model['test']  # Example: Vector for the word 'test'\n",
        "\n",
        "# Output the GloVe word vector for 'test'\n",
        "print(\"\\n GloVe Embedding for 'test':\", glove_word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YEdHSxhJMgw",
        "outputId": "57d83965-5165-44e6-aba5-67d3522f65fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " GloVe Embedding for 'test': [ 0.13175  -0.25517  -0.067915  0.26193  -0.26155   0.23569   0.13077\n",
            " -0.011801  1.7659    0.20781   0.26198  -0.16428  -0.84642   0.020094\n",
            "  0.070176  0.39778   0.15278  -0.20213  -1.6184   -0.54327  -0.17856\n",
            "  0.53894   0.49868  -0.10171   0.66265  -1.7051    0.057193 -0.32405\n",
            " -0.66835   0.26654   2.842     0.26844  -0.59537  -0.5004    1.5199\n",
            "  0.039641  1.6659    0.99758  -0.5597   -0.70493  -0.0309   -0.28302\n",
            " -0.13564   0.6429    0.41491   1.2362    0.76587   0.97798   0.58507\n",
            " -0.30176 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset for sentiment analysis and spam detection (updated)\n",
        "sample_text = dataset[0][:500]  # First 500 characters from the dataset\n",
        "\n",
        "# Tokenization using NLTK, spaCy, and BERT (already done)\n",
        "# Preprocessing and vectorization with TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "# Example sample dataset for sentiment analysis and spam detection\n",
        "texts = ['I love this product!', 'This is the worst purchase I have ever made.', 'It is okay, not bad, but not great either.']\n",
        "labels = ['positive', 'negative', 'neutral']  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit and transform the training data, transform the test data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Zz-xnGjrLGP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Full movie review dataset\n",
        "file_ids = movie_reviews.fileids()\n",
        "texts = [movie_reviews.raw(file_id) for file_id in file_ids]\n",
        "labels = ['positive' if file_id.startswith('pos') else 'negative' for file_id in file_ids]\n"
      ],
      "metadata": {
        "id": "sNaZdcw9JoQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "i8rjaB_HidBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = labels\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "classifiers = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM\": LinearSVC(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"\\n Classification Report for {name}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0swZ7bMEJpzX",
        "outputId": "d3adeabc-f311-452c-d8b9-8cba2d7f4097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Classification Report for Naive Bayes:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.83      0.81       302\n",
            "    positive       0.82      0.78      0.80       298\n",
            "\n",
            "    accuracy                           0.81       600\n",
            "   macro avg       0.81      0.80      0.80       600\n",
            "weighted avg       0.81      0.81      0.80       600\n",
            "\n",
            "\n",
            " Classification Report for SVM:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.80      0.81       302\n",
            "    positive       0.81      0.82      0.81       298\n",
            "\n",
            "    accuracy                           0.81       600\n",
            "   macro avg       0.81      0.81      0.81       600\n",
            "weighted avg       0.81      0.81      0.81       600\n",
            "\n",
            "\n",
            " Classification Report for Logistic Regression:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.82      0.82       302\n",
            "    positive       0.82      0.82      0.82       298\n",
            "\n",
            "    accuracy                           0.82       600\n",
            "   macro avg       0.82      0.82      0.82       600\n",
            "weighted avg       0.82      0.82      0.82       600\n",
            "\n",
            "\n",
            " Classification Report for Decision Tree:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.64      0.64       302\n",
            "    positive       0.64      0.64      0.64       298\n",
            "\n",
            "    accuracy                           0.64       600\n",
            "   macro avg       0.64      0.64      0.64       600\n",
            "weighted avg       0.64      0.64      0.64       600\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}